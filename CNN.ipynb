{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "base_dir = r'./DataClassification/Split_dataset'\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "validation_dir = os.path.join(base_dir, 'Valid')\n",
    "\n",
    "# Directory with our training sclerosed pictures\n",
    "train_scler_dir = os.path.join(train_dir, 'sclerosed')\n",
    "\n",
    "# Directory with our training normal pictures\n",
    "train_norm_dir = os.path.join(train_dir, 'normal')\n",
    "\n",
    "# Directory with our validation sclerosed pictures\n",
    "validation_scler_dir = os.path.join(validation_dir, 'sclerosed')\n",
    "\n",
    "# Directory with our validation normal pictures\n",
    "validation_norm_dir = os.path.join(validation_dir, 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scler_fnames = os.listdir(train_scler_dir)\n",
    "print(train_scler_fnames[:10])\n",
    "\n",
    "train_norm_fnames = os.listdir(train_norm_dir)\n",
    "train_norm_fnames.sort()\n",
    "print(train_norm_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training sclerose images:', len(os.listdir(train_scler_dir)))\n",
    "print('total training normal images:', len(os.listdir(train_norm_dir)))\n",
    "print('total validation sclerose images:', len(os.listdir(validation_scler_dir)))\n",
    "print('total validation normal images:', len(os.listdir(validation_norm_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# The input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
    "# the three color channels: R, G, and B\n",
    "img_input = layers.Input(shape=(150, 150, 3))\n",
    "\n",
    "# First convolution extracts 16 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Second convolution extracts 32 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Third convolution extracts 64 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Convolution2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Flatten feature map to a 1-dim tensor\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "# Create output layer with a single node and sigmoid activation\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Configure and compile the model\n",
    "model = Model(img_input, output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
    "# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,)\n",
    "\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 32 using val_datagen generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      epochs=epochs,\n",
    "      validation_data=validation_generator,\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve a list of accuracy results on training and validation data sets for each training epoch\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Retrieve a list of list results on training and validation data sets for each training epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylim(0, 1)  # Set the y-axis range from 0 to 1\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a Keras model for ease of use later\n",
    "model.save('CNN_P_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "# Path to the test directory containing subdirectories \"Normal\" and \"Sclerose\"\n",
    "test_dir = r'./DataClassification/Split_dataset/Test'\n",
    "\n",
    "# List of classes\n",
    "class_names = ['normal', 'sclerosed']\n",
    "\n",
    "# Initializing counters for accuracy calculation\n",
    "total_images = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "# Loop through the test directories (Normal and Sclerose)\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(test_dir, class_name)\n",
    "    for img_file in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_file)\n",
    "        \n",
    "        # Load the image and resize it to the expected size (150x150)\n",
    "        img = image.load_img(img_path, target_size=(150, 150))\n",
    "        \n",
    "        # Convert the image to a numpy array\n",
    "        img_array = image.img_to_array(img)\n",
    "        \n",
    "        # Expand the dimensions of the image to fit the expected batch size by the model\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Normalize pixel values to be in the range [0, 1]\n",
    "        img_array = img_array / 255.0\n",
    "        \n",
    "        # Make predictions with the model\n",
    "        prediction = model.predict(img_array, verbose=0)\n",
    "        \n",
    "        # Determine the true class of the image\n",
    "        true_class = 1 if class_name == 'sclerosed' else 0\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        total_images += 1\n",
    "        if (prediction > 0.5 and true_class == 1) or (prediction <= 0.5 and true_class == 0):\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        # Display prediction and true class for each image\n",
    "        # print(f\"Image {img_file} : Prediction : {class_names[int(prediction[0] > 0.5)]}, True Class : {class_name}\")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = correct_predictions / total_images if total_images > 0 else 0\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image as tf_image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load a model (Assuming a saved TensorFlow model)\n",
    "model = load_model(f'CNN_P_model.keras')\n",
    "# model.summary()\n",
    "\n",
    "# Preprocess the image for the model (This needs to be adapted to your model's requirements)\n",
    "def preprocess_image(img):\n",
    "    img = img.resize((150, 150))  # Example for a model expecting 224x224 input\n",
    "    img = tf_image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Get the list of images\n",
    "dir_list = os.listdir('./Classify')\n",
    "\n",
    "sclerosed = []\n",
    "normal = 0\n",
    "\n",
    "# Process images by directory\n",
    "for dir in dir_list:\n",
    "    image_list = os.listdir(f'./Classify/{dir}')\n",
    "\n",
    "    for image_name in image_list:\n",
    "        # Load the image\n",
    "        img = Image.open(f'./Classify/{dir}/{image_name}')\n",
    "\n",
    "        # Preprocess the image\n",
    "        img = preprocess_image(img)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model.predict(img, verbose=0)\n",
    "\n",
    "        # Get the predicted class by rounding the output probability\n",
    "        predicted = np.round(output)\n",
    "\n",
    "        if predicted:\n",
    "            print(f'Image \\t{image_name[24:-5]} \\tof {dir} is predicted to be a sclerosed glomerulus')\n",
    "            sclerosed.append(image_name)\n",
    "        else:\n",
    "            normal += 1\n",
    "\n",
    "print(f\"Total sclerosed: \\t{len(sclerosed)}\")\n",
    "\n",
    "# Save the list of sclerosed images as csv\n",
    "import csv\n",
    "\n",
    "with open(f'CNN_model_sclerosed.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for image in sclerosed:\n",
    "        writer.writerow([image])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
